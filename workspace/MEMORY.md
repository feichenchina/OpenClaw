# MEMORY.md - 长期记忆

> 这是我的核心知识库，记录重要的知识、经验和洞察。
> 每日日志在 `memory/` 目录，学习总结在 `learnings/` 目录。

## 🧠 核心知识库

### 技术知识
（记录学到的重要技术点）

#### 大模型推理硬件（2026-03-01整理）
- **核心规律**：推理是**内存带宽密集型**，选芯片看带宽（HBM GB/s），不是看FLOPS
- **主流硬件梯队**：
  - 旗舰：NVIDIA H100 SXM5（3.35 TB/s, ~$30K–40K）
  - 主流：NVIDIA A100 80G（2.0 TB/s, ~$10K–15K）
  - 推理优化：L40S（864 GB/s, ~$8K–10K）、AWS Inferentia2、Groq LPU
  - 低成本：RTX 4090（1.0 TB/s, ~$1.8K）、CPU大内存方案
  - 国产：华为昇腾910B（1.6 TB/s, ~¥20万–30万）
- **核心瓶颈**：
  1. 内存带宽（Decode阶段MBU，最根本）
  2. 显存容量（KV Cache + 权重）
  3. 多卡通信（张量并行的All-Reduce开销）
  4. 首Token延迟（Prefill计算密集 vs Decode带宽密集的异构矛盾）
  5. 功耗/散热（H100集群~10kW/节点，液冷必须）
- **量化**：INT8/INT4量化将显存减半，是降低门槛最有效的工程手段
- **详情见**：`memory/2026-03-01.md`

### 踩坑记录
（记录遇到的问题和解决方案）

### 经验教训
（记录可复用的经验）

### 系统配置
- 2026年2月10日: OpenClaw 飞书渠道配置成功，验证了消息双向通信功能

## 👤 关于用户
- **名字**：
- **位置**：
- **偏好**：

## 📁 重要路径
（记录常用的文件和目录路径）

---

*最后更新：2026-03-01*
